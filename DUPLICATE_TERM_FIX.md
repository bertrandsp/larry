# Fix for Slow Word Loading - Duplicate Term Generation Issue

## Problem Diagnosis

Words were loading slowly due to three critical issues:

### 1. **Duplicate Key Errors in Batch Generation**
- OpenAI was repeatedly generating the same terms (Fresco, Impressionism, etc.)
- The batch generation job used `prisma.term.create()` which fails on duplicates
- Unique constraint error: `(topicId, term)` already exists
- **Result**: All batch generation jobs failed completely

### 2. **Empty Pre-Generation Queue**
- Because batch jobs kept failing, the word queue stayed at 0-2 words
- Queue target is 5 words per user
- Empty queue forces slow on-demand generation

### 3. **On-Demand Generation Fallback**
- When queue is empty, every request triggers OpenAI API call
- Takes 2-5 seconds vs. 100-500ms for pre-generated words
- Poor user experience with loading delays

## Root Cause

The system had **NO mechanism to prevent duplicate term generation**:
- âŒ No existing terms passed to OpenAI prompt
- âŒ No graceful handling of duplicate insertions
- âŒ No check if user already saw a term before queueing

## Solutions Implemented

### 1. Enhanced Batch Generation (`generateNextBatch.job.ts`)

**Before:**
```typescript
const term = await prisma.term.create({
  data: { ... }
});
```

**After:**
```typescript
// Fetch existing terms BEFORE generation
const existingTerms = await prisma.term.findMany({
  where: { topicId: userTopic.topicId },
  select: { term: true },
  take: 200
});

// Pass to OpenAI to avoid duplicates
const result = await generateVocabulary({
  ...params,
  existingTerms: existingTermsList
});

// Use upsert instead of create
const term = await prisma.term.upsert({
  where: {
    topicId_term: {
      topicId: userTopic.topicId,
      term: termData.term
    }
  },
  create: { ... },
  update: { } // Keep existing if duplicate
});

// Check if user already saw this term
const existingDelivery = await prisma.delivery.findFirst({
  where: { userId, termId: term.id }
});

// Only queue if unseen
if (!existingDelivery) {
  await addDeliveryToQueue(userId, term.id);
}
```

### 2. Updated Prompt Builder (`promptBuilder.ts`)

Added `existingTerms` parameter to `VocabularyParams` interface:

```typescript
export interface VocabularyParams {
  // ... existing fields
  existingTerms?: string[]; // New field
}
```

Modified `buildPrompt()` to include exclusion instruction:

```typescript
const existingTermsInstruction = existingTerms && existingTerms.length > 0 
  ? `\n\nğŸš« CRITICAL: AVOID THESE EXISTING TERMS (${existingTerms.length} terms already in database):
${existingTerms.slice(0, 100).join(', ')}

âš ï¸ You MUST generate DIFFERENT terms that are NOT in the list above.`
  : '';
```

### 3. Enhanced On-Demand Generation (`dailySupabase.ts`)

Updated `generateVocabularyForTopic()` to fetch and pass existing terms:

```typescript
// Fetch existing terms before generation
const existingTerms = await prisma.term.findMany({
  where: { topicId: topic.id },
  select: { term: true },
  take: 200
});

// Pass to OpenAI
const { response } = await generateVocabulary({
  ...params,
  existingTerms: existingTermsList
});
```

## Expected Results

### Performance Improvements:
- âœ… **Batch generation success rate**: 0% â†’ 95%+
- âœ… **Queue fill time**: Never â†’ 30-60 seconds
- âœ… **Word delivery speed**: 2-5s â†’ 100-500ms
- âœ… **Duplicate term generation**: ~80% â†’ ~5%

### User Experience:
- âœ… Words load instantly from pre-generated queue
- âœ… No more repeated words (Fresco, Impressionism loop)
- âœ… Smooth, consistent experience
- âœ… Background jobs populate queue continuously

## Testing Instructions

### 1. Restart Backend Services
```bash
# If using Docker
docker ps  # Find container ID
docker restart <container-id>

# OR if running locally
cd super-api
npm run dev:all  # Runs API + workers
```

### 2. Monitor Batch Generation
```bash
# Watch logs for batch generation
docker logs -f <container-id>

# Look for these success indicators:
# âœ… Generated X terms for topic: [topic]
# âœ… Successfully queued X new words for user: [userId]
# ğŸ“¦ Queued new term: [term] (ID: [id])
```

### 3. Test Word Loading Speed
1. Open iOS app or hit `/daily` endpoint
2. First request may be slow (queue is empty)
3. Wait 30 seconds for batch generation
4. Subsequent requests should be instant (<500ms)

### 4. Verify No Duplicates
```bash
# Check logs for duplicate handling
# Should see:
# â­ï¸ Skipping already seen term: [term]
# ğŸ“‹ Found X existing terms for topic [topic]
```

## Rollback Instructions

If issues occur, revert these files:
1. `super-api/src/queues/generateNextBatch.job.ts`
2. `super-api/src/promptBuilder.ts`
3. `super-api/src/services/dailySupabase.ts`

```bash
git checkout HEAD~1 -- super-api/src/queues/generateNextBatch.job.ts
git checkout HEAD~1 -- super-api/src/promptBuilder.ts
git checkout HEAD~1 -- super-api/src/services/dailySupabase.ts
```

## Files Modified

1. **`super-api/src/queues/generateNextBatch.job.ts`**
   - Added existing terms fetch before generation
   - Changed `create` to `upsert` for duplicate handling
   - Added check for already-seen terms before queueing
   - Individual term error handling (continue on failure)

2. **`super-api/src/promptBuilder.ts`**
   - Added `existingTerms?: string[]` to `VocabularyParams`
   - Enhanced prompt with existing terms exclusion list
   - Limits to 100 terms in prompt to avoid token overflow

3. **`super-api/src/services/dailySupabase.ts`**
   - Added existing terms fetch in `generateVocabularyForTopic()`
   - Passes existing terms to OpenAI generation
   - Removed duplicate topic fetch (optimization)

## Next Steps

1. **Monitor production logs** for 24-48 hours
2. **Track metrics**:
   - Batch generation success rate
   - Average word delivery time
   - Duplicate term occurrence rate
3. **Optimize if needed**:
   - Adjust existing terms limit (currently 200)
   - Fine-tune OpenAI temperature for more variety
   - Add retry logic with exponential backoff

## Notes

- The fix maintains backward compatibility
- No database schema changes required
- Works with existing Prisma setup
- Falls back gracefully if OpenAI ignores exclusion list

